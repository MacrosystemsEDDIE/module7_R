---
title: "Macrosystems EDDIE Module 7: Using Data to Improve Ecological Forecasts"
author: "Mary Lofton, Tadhg Moore, Quinn Thomas, Cayelan Carey"
date: "`r Sys.Date()`"
output:
  github_document: default
---

## Purpose of this R Markdown

This R Markdown contains code to reproduce the basic functionality of "Macrosystems EDDIE Module 7: Using Data to Improve Ecological Forecasts" outside of R Shiny. The code can be used by students to better understand what is happening "under the hood" of the Shiny app, which can be found at the following link:  
https://macrosystemseddie.shinyapps.io/module7/. 

Alternatively, students can complete this version of the module instead of the Shiny app version. 

## Summary

Focal question for this module:   
**How can we use data to improve ecological forecasts?**

To be useful for management, ecological forecasts need to be both accurate enough for managers to be able to rely on them for decision-making and include a representation of forecast uncertainty, so managers can properly interpret the probability of future events. To improve forecast accuracy, we can update forecasts with observational data once they become available, a process known as **data assimilation.** Recent improvements in environmental sensor technology and an increase in the number of sensors deployed in ecosystems have resulted in an increase in the availability of data for assimilation to help develop and improve forecasts for natural resource management. In this module, you will develop an autoregressive model of primary productivity and use the model to generate forecasts. You will then explore how assimilating data at different temporal frequencies (e.g., daily, weekly) and with different levels of observation uncertainty affects forecast accuracy. 

## Learning Outcomes
1. Define data assimilation.    
2. Generate an ecological forecast for primary productivity.    
3. Describe how to assess ecological forecast accuracy.     
4. Describe how data assimilation affects forecast accuracy and uncertainty.    
5. Explain how updating models with data collected at different time scales (e.g., daily, weekly) and with different levels of associated uncertainty affects ecological forecasts.     

## Key Concepts

### What is data assimilation?

Data assimilation is the process of updating models with data. In ecological forecasting, data assimilation is the process of updating ecological forecasting models with new environmental data as they become available.

### How does the amount of uncertainty in model predictions and data affect the process of data assimilation?

The amount of uncertainty in model predictions and data determines how much we adjust our forecasts based on new observations. For example, if we observe a new data point and we have low observation uncertainty, our forecast starting conditions will be adjusted to closely correspond to the new observation. If we observe a new data point and we have high observation uncertainty, our forecast starting conditions will not be adjusted as much.

### How does the frequency of observations affect data assimilation?

More frequent observations allow us to update our forecast models more often, potentially improving forecast accuracy.

## Overview

In this module, we will generate forecasts of lake chlorophyll-a for 1-10 days into the future. First, we will generate a 10-day forecast that does not assimilate any data. This will involve the following steps:  

1. Read in and visualize chlorophyll-a data from Lake Barco, FL, USA.  
2. Explore autocorrelation of Lake Barco chlorophyll-a data.    
3. Fit an autoregressive forecast model.   
4. Specify a distribution of forecast **initial conditions** (starting conditions). 
5. Generate a 10-day forecast with no data assimilation. 
6. Assess forecast accuracy.    

Next, we will explore the effect of **data assimilation** on forecast accuracy by conducting two data assimilation experiments. First, we will assimilate data at different temporal frequencies (e.g., daily vs. weekly) and assess the effect on forecast accuracy. Second, we will assimilate data with different levels of observation uncertainty (e.g., high vs. low observation uncertainty) and assess the effect on forecast accuracy. 

7. Assimilate data at frequencies ranging from once a week to once a day.    
8. Assess the effect of data assimilation frequency on forecast accuracy.    
9. Assimilate data with different levels of observation uncertainty.   
10. Assess the effect of observation uncertainty on forecast accuracy.   

Finally, you will be asked to summarize what you have learned about how to use data to improve ecological forecasts, and explain how data assimilation frequency and observation uncertainty are likely to affect forecast accuracy.

There are a total of XX questions embedded throughout this module, many of which parallel (and in some cases are identical to) questions in the R Shiny app version of the module. Questions which are identical to those in the Shiny app will be indicated with **(Shiny)**, while questions unique to this RMarkdown will be indicated with **(Rmd)**. Note that question numbers will differ between the RMarkdown and the Shiny app, even if the question text is the same. Please see the module rubric for possible points per question and confirm with your instructor whether and how the module will be graded.  

## Think About It!

**Q.1 (Shiny)** What is meant by the term 'data assimilation' in the context of ecological forecasting?

**Answer Q.1**



**Q.2 (Shiny)** How do you think the process of integrating the most recently observed data into models can improve forecasts?

**Answer Q.2**


## Set-up

We will install and load some packages and functions that are needed to run the module code. If you do not currently have the packages below downloaded for RStudio, you will need to install them first using the `install.packages()` function.

```{r, echo=FALSE, message = FALSE, warning=FALSE}
# install.packages("tidyverse")
# install.packages("lubridate")
# install.packages("mvtnorm")
# install.packages("zoo")
library(tidyverse)
library(lubridate)
library(mvtnorm)
library(zoo)

source("./Rmd_functions.R")
```

## 1. Read in and visualize data from Lake Barco, FL, USA

Lake Barco is one of the lake sites in the U.S. National Ecological Observatory Network (NEON). Please refer to https://www.neonscience.org/field-sites/barc to learn more about this site.

**Q.3 (Shiny)** Use the website linked above to fill out information about Lake Barco:

**Answer Q.3**

Four letter site identifier:  
Latitude:  
Longitude:  
Lake area (km2):  
Elevation (m):  

## Chlorophyll-a in lakes

insert section here about why chl-a matters

**Q.4 chl-a**

**Answer Q.4**



Read in and view lake chlorophyll-a data. We will rename the columns of our dataframe, and use the `cumsum()` function to filter out several rows with NA chlorophyll-a values at the beginning of our dataset. Here, the `cumsum()` function returns, at each row, the cumulative number of non-NA chl-a values up to that row. This allows us to filter out NA values at the beginning of the dataset, because as soon as the first non-NA chl-a value is observed, the `cumsum()` function will return a value greater than 0.
```{r}
lake_data <- read_csv("./data/neon/BARC_chla_microgramsPerLiter.csv", show_col_types = FALSE) %>%
  rename(datetime = Date, chla = V1) %>%
  filter(cumsum(!is.na(chla)) > 0) %>%
  mutate(chla = ifelse(chla < 0, 0, chla))

head(lake_data)
```

Plot a timeseries of chlorophyll-a observations at Lake Barco.
```{r}
plot_chla_obs(lake_data)
```

## 2. Explore autocorrelation of Lake Barco chlorophyll-a data.    

What is autocorrelation?
What is a lag?
Notice that we are interpolating using na.approx
Show plot of chla vs chla-lag and ask students if there is a relationship
Give equation for autocorrelation
Calculate autocorrelation between chla and chla-lag and ask students to interpret
What is partial autocorrelation and why are we using that? Don't give equation; just explain that it removes effects of observations between chla and chla at a particular lag and give R function
Describe pacf function and arguments
Calculate and plot pacf
Ask questions about plot interpretation
```{r}
forecast_start_date <- "2020-09-25"

autocorrelation_data <- lake_data %>%
    filter(datetime < forecast_start_date) %>%
    mutate(chla = na.approx(chla, na.rm = F)) %>% 
    mutate(chla_lag = lag(chla)) %>%
    filter(complete.cases(.))

ggplot(data = autocorrelation_data, aes(x = chla_lag, y = chla))+
  geom_point()+
  theme_bw()

autocorrelation_lag_1 = round(sum((autocorrelation_data$chla - mean(autocorrelation_data$chla))*(autocorrelation_data$chla_lag - mean(autocorrelation_data$chla)))/sum((autocorrelation_data$chla - mean(autocorrelation_data$chla))^2),2)

acf_list <- acf(autocorrelation_data$chla, plot = FALSE)

acf_plot_data <- tibble(Lag = acf_list$lag,
                        ACF = round(acf_list$acf, 2))

ggplot(data = acf_plot_data, aes(x = Lag, y = ACF))+
  geom_bar(stat = "identity")+
  theme_bw()

pacf_list <- acf(autocorrelation_data$chla, type = c("partial"), plot = FALSE)

pacf_plot_data <- tibble(Lag = pacf_list$lag,
                         Partial_ACF = round(pacf_list$acf, 2))

ggplot(data = pacf_plot_data, aes(x = Lag, y = Partial_ACF))+
  geom_bar(stat = "identity")+
  theme_bw()
```

## 3. Fit an autoregressive forecast model.   
Explain ar.ols and why we want to use that function.
Explain arguments of ar.ols function
Explain coefficients of model output (give model equation)
Explain why we are logging chl-a data (to avoid negative values)
```{r}
model_data <- autocorrelation_data %>%
  mutate(log_chla = log(chla + 0.001))

ar_model <- ar.ols(model_data$log_chla, order.max = 1, aic = FALSE,
                     intercept = TRUE, demean = TRUE)
ar1 = ar_model$ar
chla_mean = ar_model$x.mean
intercept = ar_model$x.intercept
params_se <- ar_model$asy.se.coef

mod <- pred_AR_model(ar_model = ar_model,
                     chla = model_data$log_chla)

err <- mean(exp(mod$chla_pred) - model_data$chla, na.rm = TRUE) 
rmse <- round(sqrt(mean((mod$chla_pred - autocorrelation_data$chla)^2, na.rm = TRUE)), 2)

model_fit_plot_data <- tibble(date = model_data$datetime,
                              chla = model_data$chla,
                              model = exp(mod$chla_pred))

mod_predictions_chla(model_fit_plot_data)

```
## 4. Specify a distribution of forecast **initial conditions** (starting conditions). 
**Initial conditions** are the starting conditions of your model when you generate a forecast. **Initial conditions uncertainty** refers to uncertainty arising because the initial conditions are not precisely known or because the calculations cannot be performed with the precise initial conditions.

Even though we have measurements of chlorophyll-a from our lake, we know that chlorophyll-a varies throughout the day so this measurement might not capture exactly the chlorophyll-a in our lake at this time. Additionally, there may be observation error in our chlorophyll-a measurements.

To account for initial conditions uncertainty we can generate a distribution around the initial condition of chlorophyll-a and then run our model with slightly different initial conditions.

Generate a distribution of initial conditions for your forecast using the current chlorophyll-a (`curr_chla`) and a standard deviation of 0.5 micrograms per liter (`ic_sd`).

Need to explain rnorm function and arguments

```{r}
curr_chla <- log(lake_data %>%
  filter(datetime == forecast_start_date) %>%
  pull(chla))

high_frequency_data <- # pull in 1 month of sub-daily data in the fall of 2019 from BARCO, group by date and find SD on log scale to inform ic_sd; use code from compiling data for Abhilash's project as starting point
  
ic_sd <- 
ic_uc <- rnorm(n = 1000, mean = curr_chla, sd = ic_sd)
```

Plot the distribution around your initial condition. 

```{r}
plot_ic_dist(curr_chla, ic_uc)
```

## 5. Generate a 10-day forecast with no data assimilation. 
Right now this needs troubleshooting; I think the issue is that I need to generate a reasonable obs_sd on a log scale for chl-a; see suggested methods for this above
```{r}
#format observation data file depending on selected frequency of data assimilation
obs_data <- create_data_assim_inputs(chla_assimilation_frequency = 11,
                                     lake_data = lake_data,
                                     start_date = forecast_start_date)
n_en = 100 # how many ensemble members 
#run the forecast!
est_out = EnKF(n_en = n_en, 
           start = '2020-09-25', # start date 
           stop = '2020-10-05', # stop date
           time_step = 'days',
           obs_file = obs_data,
           n_states_est = 1, 
           n_params_est = 3,
           n_params_obs = 0,
           param_init = c(intercept, ar1, chla_mean), 
           obs_sd = ,#sd for chl-a 
           param_sd = c(0, params_se$ar, params_se$x.mean),#for mod params
           state_names = c("chla"),
           yini = curr_chla,
           model = ar_model)
#plot forecast output
plot_chla(est_out = est_out, lake_data = lake_data, obs_file = obs_file, start = "2020-09-25", stop = "2020-10-05", n_en = n_en) 
pred_v_obs_chla(est_out = est_out, lake_data = lake_data)
rmse(est_out = est_out, lake_data = lake_data)
```
### Run forecast with chl-a data assimilated every week  
Notice that the freq_chla argument in the create_data_assim_inputs file has been adjusted to 7. If chl-a data is not available on the 7th, 14th, 21st.... days, the value for that day will be NA and the forecast function will proceed without assimilating chl-a data for that day.
```{r}
#format observation data file depending on selected frequency of data assimilation
obs_file <- create_data_assim_inputs(freq_chla = 4,
                                     lake_data = lake_data,
                                     start_date = start_date)
n_en = 100 # how many ensemble members 
#run the forecast!
est_out = EnKF(n_en = n_en, 
           start = '2020-09-25', # start date 
           stop = '2020-10-05', # stop date
           time_step = 'days',
           obs_file = obs_file,
           n_states_est = 1, 
           n_params_est = 3,
           n_params_obs = 0,
           param_init = c(mod$intercept, mod$ar1, mod$mean), 
           obs_cv = c(0.05),#cv for chl-a 
           param_cv = c(0.1, 0.1, 0.1),#for mod params
           init_cond_cv = c(0.05),#cv for chl-a 
           state_names = c("chla"),
           yini = yini)
#plot forecast output
plot_chla(est_out = est_out, lake_data = lake_data, start = "2020-09-25", stop = "2020-10-05", n_en = n_en, obs_file = obs_file) 
pred_v_obs_chla(est_out = est_out, lake_data = lake_data)
rmse(est_out = est_out, lake_data = lake_data)
```

## Objective 8: Explore data frequency
This objective asks students to adjust the observation frequency (and therefore the assimilation frequency) of chl-a data. The learning outcome of the objective is for students to understand that more frequent data assimilation may permit for more accurate forecasts, as models closely track real-world conditions. Below are two examples: one where chla is assimilated low frequency (every 7 days) and one where chl-a is assimilated at high frequency (daily).  

### Low chl-a assimilation frequency
```{r}
#format observation data file depending on selected frequency of data assimilation
obs_file <- create_data_assim_inputs(freq_chla = 7,
                                     lake_data = lake_data,
                                     start_date = start_date)
n_en = 100 # how many ensemble members 
#run the forecast!
est_out = EnKF(n_en = n_en, 
           start = '2020-09-25', # start date 
           stop = '2020-10-05', # stop date
           time_step = 'days',
           obs_file = obs_file,
           n_states_est = 1, 
           n_params_est = 3,
           n_params_obs = 0,
           param_init = c(mod$intercept, mod$ar1, mod$mean), 
           obs_cv = c(0.05),#cv for chl-a 
           param_cv = c(0.1, 0.1, 0.1),#for mod params
           init_cond_cv = c(0.05),#cv for chl-a 
           state_names = c("chla"),
           yini = yini)
#plot forecast output
plot_chla(est_out = est_out, lake_data = lake_data, start = "2020-09-25", stop = "2020-10-05", n_en = n_en, obs_file = obs_file) 
pred_v_obs_chla(est_out = est_out, lake_data = lake_data)
rmse(est_out = est_out, lake_data = lake_data)
```

### High chl-a assimilation frequency
```{r}
#format observation data file depending on selected frequency of data assimilation
obs_file <- create_data_assim_inputs(freq_chla = 1,
                                     lake_data = lake_data,
                                     start_date = start_date)
n_en = 100 # how many ensemble members 
#run the forecast!
est_out = EnKF(n_en = n_en, 
           start = '2020-09-25', # start date 
           stop = '2020-10-05', # stop date
           time_step = 'days',
           obs_file = obs_file,
           n_states_est = 1, 
           n_params_est = 3,
           n_params_obs = 0,
           param_init = c(mod$intercept, mod$ar1, mod$mean), 
           obs_cv = c(0.05),#cv for chl-a 
           param_cv = c(0.1, 0.1, 0.1),#for mod params
           init_cond_cv = c(0.05),#cv for chl-a 
           state_names = c("chla"),
           yini = yini)
#plot forecast output
plot_chla(est_out = est_out, lake_data = lake_data, start = "2020-09-25", stop = "2020-10-05", n_en = n_en, obs_file = obs_file) 
pred_v_obs_chla(est_out = est_out, lake_data = lake_data)
rmse(est_out = est_out, lake_data = lake_data)
```

## Objective 9: Explore observation uncertainty  
This objective will ask students to adjust the observation uncertainty associated with chl-a and nitrate data. The learning outcome of the objective is for students to understand that the amount of uncertainty associated with observations determines how much the model corrects itself to align with those observations. Below are two examples: one when observation uncertainty for both variables is low, and one when it is high.  

### Low observation uncertainty
```{r}
#format observation data file depending on selected frequency of data assimilation
obs_file <- create_data_assim_inputs(freq_chla = 1,
                                     lake_data = lake_data,
                                     start_date = start_date)
#run the forecast!
est_out = EnKF(n_en = n_en, 
           start = '2020-09-25', # start date 
           stop = '2020-10-05', # stop date
           time_step = 'days',
           obs_file = obs_file,
           n_states_est = 1, 
           n_params_est = 3,
           n_params_obs = 0,
           param_init = c(mod$intercept, mod$ar1, mod$mean), 
           obs_cv = c(0.01),#cv for chl-a 
           param_cv = c(0.1, 0.1, 0.1),#for mod params
           init_cond_cv = c(0.01),#cv for chl-a 
           state_names = c("chla"),
           yini = yini)
#plot forecast output
plot_chla(est_out = est_out, lake_data = lake_data, start = "2020-09-25", stop = "2020-10-05", n_en = n_en, obs_file = obs_file) 
pred_v_obs_chla(est_out = est_out, lake_data = lake_data)
rmse(est_out = est_out, lake_data = lake_data)
```
### High observation uncertainty
```{r}
#format observation data file depending on selected frequency of data assimilation
obs_file <- create_data_assim_inputs(freq_chla = 1,
                                     lake_data = lake_data,
                                     start_date = start_date)
#run the forecast!
est_out = EnKF(n_en = n_en, 
           start = '2020-09-25', # start date 
           stop = '2020-10-29', # stop date
           time_step = 'days',
           obs_file = obs_file,
           n_states_est = 1, 
           n_params_est = 3,
           n_params_obs = 0,
           param_init = c(mod$intercept, mod$ar1, mod$mean), 
           obs_cv = c(0.2),#cv for chl-a 
           param_cv = c(0.1, 0.1, 0.1),#for mod params
           init_cond_cv = c(0.2),#cv for chl-a 
           state_names = c("chla"),
           yini = yini)
#plot forecast output
plot_chla(est_out = est_out, lake_data = lake_data, start = "2020-09-25", stop = "2020-10-05", n_en = n_en, obs_file = obs_file) 
pred_v_obs_chla(est_out = est_out, lake_data = lake_data)
rmse(est_out = est_out, lake_data = lake_data)
```


Footer